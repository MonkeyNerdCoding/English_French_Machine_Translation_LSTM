{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "59b9c404",
      "metadata": {
        "id": "59b9c404"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import spacy\n",
        "import os\n",
        "import random\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1ktXjQ0SRGVT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ktXjQ0SRGVT",
        "outputId": "d61b733d-e316-4fcb-c35b-10d7258766a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "hOeyQvzWzdB8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOeyQvzWzdB8",
        "outputId": "fa1671d1-b56a-480e-c218-10ac74f3bc23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f755dae3",
      "metadata": {
        "id": "f755dae3"
      },
      "outputs": [],
      "source": [
        "# !pip install torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bd39989",
      "metadata": {
        "id": "6bd39989"
      },
      "source": [
        "# ============================================================================\n",
        "# 1. CHUẨN BỊ DỮ LIỆU\n",
        "# ============================================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "01aa45bd",
      "metadata": {
        "id": "01aa45bd"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Vocabulary:\n",
        "    \"\"\"Xây dựng từ điển cho một ngôn ngữ\"\"\"\n",
        "    def __init__(self, max_vocab_size=10000):\n",
        "        self.max_vocab_size = max_vocab_size\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.word_freq = Counter()\n",
        "\n",
        "        # Token đặc biệt\n",
        "        self.PAD_TOKEN = '<pad>'\n",
        "        self.UNK_TOKEN = '<unk>'\n",
        "        self.SOS_TOKEN = '<sos>'\n",
        "        self.EOS_TOKEN = '<eos>'\n",
        "\n",
        "        self.pad_idx = 0\n",
        "        self.unk_idx = 1\n",
        "        self.sos_idx = 2\n",
        "        self.eos_idx = 3\n",
        "\n",
        "    def build_vocab(self, sentences):\n",
        "        \"\"\"Xây dựng từ điển từ danh sách câu\"\"\"\n",
        "        # Đếm tần suất từ\n",
        "        for sentence in sentences:\n",
        "            self.word_freq.update(sentence)\n",
        "\n",
        "        # Lấy top từ phổ biến\n",
        "        most_common = self.word_freq.most_common(self.max_vocab_size - 4)\n",
        "\n",
        "        # Thêm token đặc biệt\n",
        "        self.word2idx = {\n",
        "            self.PAD_TOKEN: self.pad_idx,\n",
        "            self.UNK_TOKEN: self.unk_idx,\n",
        "            self.SOS_TOKEN: self.sos_idx,\n",
        "            self.EOS_TOKEN: self.eos_idx\n",
        "        }\n",
        "\n",
        "        # Thêm các từ phổ biến\n",
        "        for idx, (word, freq) in enumerate(most_common, start=4):\n",
        "            self.word2idx[word] = idx\n",
        "\n",
        "        # Tạo ánh xạ ngược\n",
        "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
        "\n",
        "    def encode(self, sentence):\n",
        "        \"\"\"Chuyển câu thành list index\"\"\"\n",
        "        return [self.word2idx.get(word, self.unk_idx) for word in sentence]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        \"\"\"Chuyển list index thành câu\"\"\"\n",
        "        return [self.idx2word.get(idx, self.UNK_TOKEN) for idx in indices]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n",
        "\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    \"\"\"Dataset cho dịch máy\"\"\"\n",
        "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab):\n",
        "        self.src_sentences = src_sentences\n",
        "        self.tgt_sentences = tgt_sentences\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.src_sentences[idx]\n",
        "        tgt = self.tgt_sentences[idx]\n",
        "\n",
        "        # Encode thành indices\n",
        "        src_indices = self.src_vocab.encode(src) + [self.src_vocab.eos_idx]\n",
        "        tgt_indices = [self.tgt_vocab.sos_idx] + self.tgt_vocab.encode(tgt) + [self.tgt_vocab.eos_idx]\n",
        "\n",
        "        return torch.LongTensor(src_indices), torch.LongTensor(tgt_indices)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Collate function để xử lý padding và sorting\"\"\"\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "\n",
        "    # Sắp xếp theo độ dài giảm dần (bắt buộc cho pack_padded_sequence)\n",
        "    src_lengths = torch.LongTensor([len(s) for s in src_batch])\n",
        "    sorted_indices = src_lengths.argsort(descending=True)\n",
        "\n",
        "    src_batch = [src_batch[i] for i in sorted_indices]\n",
        "    tgt_batch = [tgt_batch[i] for i in sorted_indices]\n",
        "    src_lengths = src_lengths[sorted_indices]\n",
        "\n",
        "    # Padding\n",
        "    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
        "    tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n",
        "\n",
        "    return src_padded, src_lengths, tgt_padded"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50ad9910",
      "metadata": {
        "id": "50ad9910"
      },
      "source": [
        "# ============================================================================\n",
        "# 2. MÔ HÌNH ENCODER-DECODER\n",
        "# ============================================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a85649a7",
      "metadata": {
        "id": "a85649a7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"LSTM Encoder\"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers,\n",
        "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_lengths):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: (batch_size, src_len)\n",
        "            src_lengths: (batch_size,)\n",
        "        Returns:\n",
        "            hidden: (num_layers, batch_size, hidden_size)\n",
        "            cell: (num_layers, batch_size, hidden_size)\n",
        "        \"\"\"\n",
        "        embedded = self.dropout(self.embedding(src))  # (batch, src_len, emb_dim)\n",
        "\n",
        "        # Pack sequence\n",
        "        packed = pack_padded_sequence(embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=True)\n",
        "\n",
        "        # LSTM\n",
        "        packed_output, (hidden, cell) = self.lstm(packed)\n",
        "\n",
        "        # hidden: (num_layers, batch, hidden_size)\n",
        "        # cell: (num_layers, batch, hidden_size)\n",
        "        return hidden, cell\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"LSTM Decoder\"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers,\n",
        "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "        self.fc_out = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_token, hidden, cell):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_token: (batch_size, 1)\n",
        "            hidden: (num_layers, batch_size, hidden_size)\n",
        "            cell: (num_layers, batch_size, hidden_size)\n",
        "        Returns:\n",
        "            prediction: (batch_size, vocab_size)\n",
        "            hidden: (num_layers, batch_size, hidden_size)\n",
        "            cell: (num_layers, batch_size, hidden_size)\n",
        "        \"\"\"\n",
        "        embedded = self.dropout(self.embedding(input_token))  # (batch, 1, emb_dim)\n",
        "\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        # output: (batch, 1, hidden_size)\n",
        "\n",
        "        prediction = self.fc_out(output.squeeze(1))  # (batch, vocab_size)\n",
        "\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "\n",
        "# Thay thế class Seq2Seq.forward bằng đoạn này\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, src_lengths, tgt, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.shape[0]\n",
        "        tgt_len = tgt.shape[1]\n",
        "        tgt_vocab_size = self.decoder.vocab_size\n",
        "\n",
        "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
        "\n",
        "        # Encoder\n",
        "        hidden, cell = self.encoder(src, src_lengths)\n",
        "\n",
        "        # Nếu dùng full teacher forcing -> vectorized decoding (1 pass LSTM)\n",
        "        if teacher_forcing_ratio >= 1.0 - 1e-9:\n",
        "            # Input to decoder LSTM are the target tokens excluding the last token\n",
        "            # (we use targets as inputs when teacher forcing)\n",
        "            # tgt_in: shape (batch, tgt_len-1)\n",
        "            tgt_in = tgt[:, :-1]  # exclude final <eos> if present\n",
        "            embedded = self.decoder.embedding(tgt_in)  # (batch, seq_len, emb_dim)\n",
        "            embedded = self.decoder.dropout(embedded)\n",
        "\n",
        "            # Run decoder LSTM once for the whole sequence\n",
        "            decoder_outputs, (hidden, cell) = self.decoder.lstm(embedded, (hidden, cell))\n",
        "            # decoder_outputs: (batch, seq_len, hidden_size)\n",
        "            # Map to vocab\n",
        "            pred = self.decoder.fc_out(decoder_outputs)  # (batch, seq_len, vocab_size)\n",
        "\n",
        "            # Place predictions into outputs aligned so that outputs[:, t, :] predicts token at tgt[:, t]\n",
        "            outputs[:, 1:tgt_len, :] = pred  # note: pred corresponds to steps 1..tgt_len-1\n",
        "            # (outputs[:,0,:] stays zeros or you can set it to prediction for <sos> if you want)\n",
        "            return outputs\n",
        "\n",
        "        # Fallback: mixed or no teacher forcing -> step-by-step (cũ)\n",
        "        decoder_input = tgt[:, 0].unsqueeze(1)  # <sos>\n",
        "        for t in range(1, tgt_len):\n",
        "            prediction, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
        "            outputs[:, t, :] = prediction\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = prediction.argmax(1).unsqueeze(1)\n",
        "            decoder_input = tgt[:, t].unsqueeze(1) if teacher_force else top1\n",
        "\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4007c696",
      "metadata": {
        "id": "4007c696"
      },
      "source": [
        "# ============================================================================\n",
        "# 3. HUẤN LUYỆN\n",
        "# ============================================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "49dab472",
      "metadata": {
        "id": "49dab472"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, optimizer, criterion, clip, device):\n",
        "    \"\"\"Huấn luyện 1 epoch\"\"\"\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for src, src_lengths, tgt in dataloader:\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward\n",
        "        output = model(src, src_lengths, tgt, teacher_forcing_ratio=0.5)\n",
        "\n",
        "        # output: (batch, tgt_len, vocab_size)\n",
        "        # tgt: (batch, tgt_len)\n",
        "\n",
        "        # Bỏ <sos> token và flatten\n",
        "        output = output[:, 1:, :].reshape(-1, output.shape[-1])\n",
        "        tgt = tgt[:, 1:].reshape(-1)\n",
        "\n",
        "        # Tính loss\n",
        "        loss = criterion(output, tgt)\n",
        "\n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    \"\"\"Đánh giá trên tập validation\"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, src_lengths, tgt in dataloader:\n",
        "            src = src.to(device)\n",
        "            tgt = tgt.to(device)\n",
        "\n",
        "            # Forward (không teacher forcing)\n",
        "            output = model(src, src_lengths, tgt, teacher_forcing_ratio=0)\n",
        "\n",
        "            output = output[:, 1:, :].reshape(-1, output.shape[-1])\n",
        "            tgt = tgt[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, tgt)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9377802",
      "metadata": {
        "id": "e9377802"
      },
      "source": [
        "# ============================================================================\n",
        "# 4. DỰ ĐOÁN (INFERENCE)\n",
        "# ============================================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "801b4f7f",
      "metadata": {
        "id": "801b4f7f"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(model, sentence, src_vocab, tgt_vocab, src_tokenizer, device, max_len=50):\n",
        "    \"\"\"\n",
        "    Dịch một câu từ tiếng Anh sang tiếng Pháp\n",
        "\n",
        "    Args:\n",
        "        model: mô hình đã train\n",
        "        sentence: câu tiếng Anh (string)\n",
        "        src_vocab, tgt_vocab: từ điển\n",
        "        src_tokenizer: tokenizer cho tiếng Anh\n",
        "        device: cuda/cpu\n",
        "        max_len: độ dài tối đa câu dịch\n",
        "\n",
        "    Returns:\n",
        "        translated_sentence: câu tiếng Pháp (string)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = src_tokenizer(sentence.lower())\n",
        "    tokens = [token.text for token in tokens]\n",
        "\n",
        "    # Encode\n",
        "    indices = src_vocab.encode(tokens) + [src_vocab.eos_idx]\n",
        "    src_tensor = torch.LongTensor(indices).unsqueeze(0).to(device)  # (1, src_len)\n",
        "    src_lengths = torch.LongTensor([len(indices)])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Encoder\n",
        "        hidden, cell = model.encoder(src_tensor, src_lengths)\n",
        "\n",
        "        # Decoder\n",
        "        decoder_input = torch.LongTensor([tgt_vocab.sos_idx]).unsqueeze(0).to(device)\n",
        "        translated_indices = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            prediction, hidden, cell = model.decoder(decoder_input, hidden, cell)\n",
        "            predicted_token = prediction.argmax(1).item()\n",
        "\n",
        "            if predicted_token == tgt_vocab.eos_idx:\n",
        "                break\n",
        "\n",
        "            translated_indices.append(predicted_token)\n",
        "            decoder_input = torch.LongTensor([predicted_token]).unsqueeze(0).to(device)\n",
        "\n",
        "    # Decode\n",
        "    translated_tokens = tgt_vocab.decode(translated_indices)\n",
        "    translated_sentence = ' '.join(translated_tokens)\n",
        "\n",
        "    return translated_sentence\n",
        "\n",
        "\n",
        "def calculate_bleu(model, test_data, src_vocab, tgt_vocab, src_tokenizer, device):\n",
        "    \"\"\"Tính BLEU score trên tập test\"\"\"\n",
        "    bleu_scores = []\n",
        "    smoothing = SmoothingFunction().method1\n",
        "\n",
        "    for src_sent, tgt_sent in test_data:\n",
        "        src_text = ' '.join(src_sent)\n",
        "        translated = translate_sentence(model, src_text, src_vocab, tgt_vocab, src_tokenizer, device)\n",
        "\n",
        "        reference = [tgt_sent]\n",
        "        candidate = translated.split()\n",
        "\n",
        "        score = sentence_bleu(reference, candidate, smoothing_function=smoothing)\n",
        "        bleu_scores.append(score)\n",
        "\n",
        "    return np.mean(bleu_scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ce7a31f",
      "metadata": {
        "id": "8ce7a31f"
      },
      "source": [
        "# ============================================================================\n",
        "# 5. MAIN - SỬ DỤNG\n",
        "# ============================================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d113f40b",
      "metadata": {
        "id": "d113f40b"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"Hàm chính - ví dụ sử dụng. Ở đây thêm code load/tokenize/build vocab và tạo DataLoader.\"\"\"\n",
        "    # Cấu hình\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Hyperparameters\n",
        "    EMBEDDING_DIM = 256\n",
        "    HIDDEN_SIZE = 512\n",
        "    NUM_LAYERS = 2\n",
        "    DROPOUT = 0.5\n",
        "    LEARNING_RATE = 0.001\n",
        "    BATCH_SIZE = 64\n",
        "    NUM_EPOCHS = 10\n",
        "    CLIP = 1\n",
        "\n",
        "    # -- Đường dẫn tới thư mục dữ liệu (tương đối với notebook) --\n",
        "    # data_dir = os.path.join(os.getcwd(), 'dataset') # Original incorrect path\n",
        "    data_dir = '/content/' # Correct path to the dataset files\n",
        "\n",
        "    def load_lines(path):\n",
        "        \"\"\"Đọc file và trả về danh sách dòng (không tokenized)\"\"\"\n",
        "        lines = []\n",
        "        with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    lines.append(line)\n",
        "        return lines\n",
        "\n",
        "    def tokenize_lines(lines, tokenizer):\n",
        "        \"\"\"Tokenize danh sách câu trả về list of token lists\"\"\"\n",
        "        tokenized = []\n",
        "        for line in lines:\n",
        "            toks = [t.text for t in tokenizer(line.lower()) if t.text.strip()]\n",
        "            tokenized.append(toks)\n",
        "        return tokenized\n",
        "\n",
        "    # Tập tin theo cấu trúc workspace: dataset/<split>/(file)\n",
        "    # Updated paths to reflect the file location in /content/\n",
        "    train_en_path = os.path.join(data_dir, 'train.en')\n",
        "    train_fr_path = os.path.join(data_dir, 'train.fr')\n",
        "    val_en_path = os.path.join(data_dir, 'val.en')\n",
        "    val_fr_path = os.path.join(data_dir, 'val.fr')\n",
        "    test_en_path = os.path.join(data_dir, 'test_2018_flickr.en')\n",
        "    test_fr_path = os.path.join(data_dir, 'test_2018_flickr.fr')\n",
        "\n",
        "\n",
        "    # Load raw lines\n",
        "    train_en_lines = load_lines(train_en_path)\n",
        "    train_fr_lines = load_lines(train_fr_path)\n",
        "    val_en_lines = load_lines(val_en_path)\n",
        "    val_fr_lines = load_lines(val_fr_path)\n",
        "    test_en_lines = load_lines(test_en_path)\n",
        "    test_fr_lines = load_lines(test_fr_path)\n",
        "\n",
        "    # Tokenizers: thử load spacy models, fallback sang spacy.blank nếu model chưa cài\n",
        "    try:\n",
        "        en_tokenizer = spacy.load('en_core_web_sm')\n",
        "    except Exception:\n",
        "        # Nếu model en_core_web_sm chưa cài, dùng blank tokenizer (đơn giản)\n",
        "        print(\"Warning: en_core_web_sm not found. Using spacy.blank('en'). Please install with !python -m spacy download en_core_web_sm\")\n",
        "        en_tokenizer = spacy.blank('en')\n",
        "\n",
        "    try:\n",
        "        fr_tokenizer = spacy.load('fr_core_news_sm')\n",
        "    except Exception:\n",
        "        print(\"Warning: fr_core_news_sm not found. Using spacy.blank('fr'). Please install with !python -m spacy download fr_core_news_sm\")\n",
        "        fr_tokenizer = spacy.blank('fr')\n",
        "\n",
        "    # Tokenize tất cả\n",
        "    print('Tokenizing...')\n",
        "    train_en_tok = tokenize_lines(train_en_lines, en_tokenizer)\n",
        "    train_fr_tok = tokenize_lines(train_fr_lines, fr_tokenizer)\n",
        "    val_en_tok = tokenize_lines(val_en_lines, en_tokenizer)\n",
        "    val_fr_tok = tokenize_lines(val_fr_lines, fr_tokenizer)\n",
        "    test_en_tok = tokenize_lines(test_en_lines, en_tokenizer)\n",
        "    test_fr_tok = tokenize_lines(test_fr_lines, fr_tokenizer)\n",
        "\n",
        "\n",
        "    # Build vocabularies (từ train set)\n",
        "    print('Building vocabularies...')\n",
        "    src_vocab = Vocabulary(max_vocab_size=10000)\n",
        "    src_vocab.build_vocab(train_en_tok)\n",
        "    tgt_vocab = Vocabulary(max_vocab_size=10000)\n",
        "    tgt_vocab.build_vocab(train_fr_tok)\n",
        "\n",
        "    # Tạo Dataset và DataLoader\n",
        "    train_dataset = TranslationDataset(train_en_tok, train_fr_tok, src_vocab, tgt_vocab)\n",
        "    val_dataset = TranslationDataset(val_en_tok, val_fr_tok, src_vocab, tgt_vocab)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    # test_data dùng cho calculate_bleu (dạng list of (src_tokens, tgt_tokens))\n",
        "    test_data = list(zip(test_en_tok, test_fr_tok))\n",
        "\n",
        "    # Sanity prints\n",
        "    print(f'Train examples: {len(train_dataset)}')\n",
        "    print(f'Val examples: {len(val_dataset)}')\n",
        "    print(f'Test examples: {len(test_data)}')\n",
        "    print('Example tokenized (src -> tgt):')\n",
        "    print(train_en_tok[0][:20])\n",
        "    print(train_fr_tok[0][:20])\n",
        "\n",
        "    # Khởi tạo mô hình (dùng kích thước từ vocab đã tạo)\n",
        "    encoder = Encoder(len(src_vocab), EMBEDDING_DIM, HIDDEN_SIZE, NUM_LAYERS, DROPOUT)\n",
        "    decoder = Decoder(len(tgt_vocab), EMBEDDING_DIM, HIDDEN_SIZE, NUM_LAYERS, DROPOUT)\n",
        "    model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "    # Optimizer và loss\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    # Use ignore_index for PAD_TOKEN in CrossEntropyLoss\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab.pad_idx) # Corrected to use tgt_vocab.pad_idx\n",
        "\n",
        "\n",
        "    # (Phần training vẫn như cũ) -- user có thể chạy training nếu muốn\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 3\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, criterion, CLIP, device)\n",
        "        val_loss = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
        "        print(f'Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}')\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping!\")\n",
        "                break\n",
        "\n",
        "    # Đánh giá BLEU\n",
        "    bleu_score = calculate_bleu(model, test_data, src_vocab, tgt_vocab, en_tokenizer, device)\n",
        "    print(f'BLEU Score: {bleu_score:.4f}')\n",
        "\n",
        "    # Ví dụ dịch\n",
        "    sentence = \"A man is riding a horse.\"\n",
        "    translation = translate_sentence(model, sentence, src_vocab, tgt_vocab, en_tokenizer, device)\n",
        "    print(f'English: {sentence}')\n",
        "    print(f'French: {translation}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "bW4znrDsw5c8",
      "metadata": {
        "id": "bW4znrDsw5c8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "376805f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "376805f2",
        "outputId": "5e1b27c0-b18d-446a-f7ea-c0c23c09192f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: fr_core_news_sm not found. Using spacy.blank('fr'). Please install with !python -m spacy download fr_core_news_sm\n",
            "Tokenizing...\n",
            "Building vocabularies...\n",
            "Train examples: 29000\n",
            "Val examples: 1014\n",
            "Test examples: 1071\n",
            "Example tokenized (src -> tgt):\n",
            "['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n",
            "['deux', 'jeunes', 'hommes', 'blancs', 'sont', 'dehors', 'près', 'de', 'buissons', '.']\n",
            "Epoch 1/10\n",
            "Train Loss: 4.933 | Val Loss: 4.712\n",
            "Epoch 2/10\n",
            "Train Loss: 3.970 | Val Loss: 4.344\n",
            "Epoch 3/10\n",
            "Train Loss: 3.548 | Val Loss: 4.071\n",
            "Epoch 4/10\n",
            "Train Loss: 3.269 | Val Loss: 3.937\n",
            "Epoch 5/10\n",
            "Train Loss: 3.023 | Val Loss: 3.897\n",
            "Epoch 6/10\n",
            "Train Loss: 2.841 | Val Loss: 3.745\n",
            "Epoch 7/10\n",
            "Train Loss: 2.678 | Val Loss: 3.698\n",
            "Epoch 8/10\n",
            "Train Loss: 2.560 | Val Loss: 3.611\n",
            "Epoch 9/10\n",
            "Train Loss: 2.428 | Val Loss: 3.640\n",
            "Epoch 10/10\n",
            "Train Loss: 2.328 | Val Loss: 3.601\n",
            "BLEU Score: 0.1279\n",
            "English: A man is riding a horse.\n",
            "French: un homme surfe un cheval .\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7FV0yyInzJPs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FV0yyInzJPs",
        "outputId": "5af2a1d7-a828-40ac-f28f-0704b1a94d36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "Vr6AP4adzMy8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vr6AP4adzMy8",
        "outputId": "cf5d4784-55a7-491d-f47a-6a36658fa3cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Nov  4 17:16:52 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P0             36W /   70W |    4850MiB /  15360MiB |     28%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "qzIX5EXbZHQu",
      "metadata": {
        "id": "qzIX5EXbZHQu"
      },
      "outputs": [],
      "source": [
        "# !python -m spacy download fr_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Mh3VsH4KZINW",
      "metadata": {
        "id": "Mh3VsH4KZINW"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (.venv)",
      "language": "python",
      "name": ".venv"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
