{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "59b9c404",
      "metadata": {
        "id": "59b9c404"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import spacy\n",
        "import os\n",
        "import random\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1ktXjQ0SRGVT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ktXjQ0SRGVT",
        "outputId": "9322c7a0-e12e-4fcb-90e2-c6679cc99c9d"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      2\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hOeyQvzWzdB8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOeyQvzWzdB8",
        "outputId": "0c06ed28-a2f8-488e-8a4b-8ec1590f8871"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f755dae3",
      "metadata": {
        "id": "f755dae3"
      },
      "outputs": [],
      "source": [
        "# !pip install torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bd39989",
      "metadata": {
        "id": "6bd39989"
      },
      "source": [
        "# ============================================================================\n",
        "# 1. CHU·∫®N B·ªä D·ªÆ LI·ªÜU\n",
        "# ============================================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01aa45bd",
      "metadata": {
        "id": "01aa45bd"
      },
      "outputs": [],
      "source": [
        "class Vocabulary:\n",
        "    \"\"\"X√¢y d·ª±ng t·ª´ ƒëi·ªÉn cho m·ªôt ng√¥n ng·ªØ\"\"\"\n",
        "    def __init__(self, max_vocab_size=10000):\n",
        "        self.max_vocab_size = max_vocab_size\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.word_freq = Counter()\n",
        "\n",
        "        # Token ƒë·∫∑c bi·ªát\n",
        "        self.PAD_TOKEN = '<pad>'\n",
        "        self.UNK_TOKEN = '<unk>'\n",
        "        self.SOS_TOKEN = '<sos>'\n",
        "        self.EOS_TOKEN = '<eos>'\n",
        "\n",
        "        self.pad_idx = 0\n",
        "        self.unk_idx = 1\n",
        "        self.sos_idx = 2\n",
        "        self.eos_idx = 3\n",
        "\n",
        "    def build_vocab(self, sentences):\n",
        "        \"\"\"X√¢y d·ª±ng t·ª´ ƒëi·ªÉn t·ª´ danh s√°ch c√¢u\"\"\"\n",
        "        # ƒê·∫øm t·∫ßn su·∫•t t·ª´\n",
        "        for sentence in sentences:\n",
        "            self.word_freq.update(sentence)\n",
        "\n",
        "        # L·∫•y top t·ª´ ph·ªï bi·∫øn\n",
        "        most_common = self.word_freq.most_common(self.max_vocab_size - 4)\n",
        "\n",
        "        # Th√™m token ƒë·∫∑c bi·ªát\n",
        "        self.word2idx = {\n",
        "            self.PAD_TOKEN: self.pad_idx,\n",
        "            self.UNK_TOKEN: self.unk_idx,\n",
        "            self.SOS_TOKEN: self.sos_idx,\n",
        "            self.EOS_TOKEN: self.eos_idx\n",
        "        }\n",
        "\n",
        "        # Th√™m c√°c t·ª´ ph·ªï bi·∫øn\n",
        "        for idx, (word, freq) in enumerate(most_common, start=4):\n",
        "            self.word2idx[word] = idx\n",
        "\n",
        "        # T·∫°o √°nh x·∫° ng∆∞·ª£c\n",
        "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
        "\n",
        "    def encode(self, sentence):\n",
        "        \"\"\"Chuy·ªÉn c√¢u th√†nh list index\"\"\"\n",
        "        return [self.word2idx.get(word, self.unk_idx) for word in sentence]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        \"\"\"Chuy·ªÉn list index th√†nh c√¢u\"\"\"\n",
        "        return [self.idx2word.get(idx, self.UNK_TOKEN) for idx in indices]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n",
        "\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    \"\"\"Dataset cho d·ªãch m√°y\"\"\"\n",
        "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab):\n",
        "        self.src_sentences = src_sentences\n",
        "        self.tgt_sentences = tgt_sentences\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.src_sentences[idx]\n",
        "        tgt = self.tgt_sentences[idx]\n",
        "\n",
        "        # Encode th√†nh indices\n",
        "        src_indices = self.src_vocab.encode(src) + [self.src_vocab.eos_idx]\n",
        "        tgt_indices = [self.tgt_vocab.sos_idx] + self.tgt_vocab.encode(tgt) + [self.tgt_vocab.eos_idx]\n",
        "\n",
        "        return torch.LongTensor(src_indices), torch.LongTensor(tgt_indices)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Collate function ƒë·ªÉ x·ª≠ l√Ω padding v√† sorting\"\"\"\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "\n",
        "    # S·∫Øp x·∫øp theo ƒë·ªô d√†i gi·∫£m d·∫ßn (b·∫Øt bu·ªôc cho pack_padded_sequence)\n",
        "    src_lengths = torch.LongTensor([len(s) for s in src_batch])\n",
        "    sorted_indices = src_lengths.argsort(descending=True)\n",
        "\n",
        "    src_batch = [src_batch[i] for i in sorted_indices]\n",
        "    tgt_batch = [tgt_batch[i] for i in sorted_indices]\n",
        "    src_lengths = src_lengths[sorted_indices]\n",
        "\n",
        "    # Padding\n",
        "    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
        "    tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n",
        "\n",
        "    return src_padded, src_lengths, tgt_padded"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fOlQ54GLBXMD",
      "metadata": {
        "id": "fOlQ54GLBXMD"
      },
      "source": [
        "# ============================================================================\n",
        "# 2. M√î H√åNH ENCODER-DECODER\n",
        "# ============================================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a85649a7",
      "metadata": {
        "id": "a85649a7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"LSTM Encoder\"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers,\n",
        "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_lengths):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: (batch_size, src_len)\n",
        "            src_lengths: (batch_size,)\n",
        "        Returns:\n",
        "            hidden: (num_layers, batch_size, hidden_size)\n",
        "            cell: (num_layers, batch_size, hidden_size)\n",
        "        \"\"\"\n",
        "        embedded = self.dropout(self.embedding(src))  # (batch, src_len, emb_dim)\n",
        "\n",
        "        # Pack sequence\n",
        "        packed = pack_padded_sequence(embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=True)\n",
        "\n",
        "        # LSTM\n",
        "        packed_output, (hidden, cell) = self.lstm(packed)\n",
        "\n",
        "        # hidden: (num_layers, batch, hidden_size)\n",
        "        # cell: (num_layers, batch, hidden_size)\n",
        "        return hidden, cell\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"LSTM Decoder\"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers,\n",
        "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "        self.fc_out = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_token, hidden, cell):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_token: (batch_size, 1)\n",
        "            hidden: (num_layers, batch_size, hidden_size)\n",
        "            cell: (num_layers, batch_size, hidden_size)\n",
        "        Returns:\n",
        "            prediction: (batch_size, vocab_size)\n",
        "            hidden: (num_layers, batch_size, hidden_size)\n",
        "            cell: (num_layers, batch_size, hidden_size)\n",
        "        \"\"\"\n",
        "        embedded = self.dropout(self.embedding(input_token))  # (batch, 1, emb_dim)\n",
        "\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        # output: (batch, 1, hidden_size)\n",
        "\n",
        "        prediction = self.fc_out(output.squeeze(1))  # (batch, vocab_size)\n",
        "\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "\n",
        "# Thay th·∫ø class Seq2Seq.forward b·∫±ng ƒëo·∫°n n√†y\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, src_lengths, tgt, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.shape[0]\n",
        "        tgt_len = tgt.shape[1]\n",
        "        tgt_vocab_size = self.decoder.vocab_size\n",
        "\n",
        "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
        "\n",
        "        # Encoder\n",
        "        hidden, cell = self.encoder(src, src_lengths)\n",
        "\n",
        "        # N·∫øu d√πng full teacher forcing -> vectorized decoding (1 pass LSTM)\n",
        "        if teacher_forcing_ratio >= 1.0 - 1e-9:\n",
        "            # Input to decoder LSTM are the target tokens excluding the last token\n",
        "            # (we use targets as inputs when teacher forcing)\n",
        "            # tgt_in: shape (batch, tgt_len-1)\n",
        "            tgt_in = tgt[:, :-1]  # exclude final <eos> if present\n",
        "            embedded = self.decoder.embedding(tgt_in)  # (batch, seq_len, emb_dim)\n",
        "            embedded = self.decoder.dropout(embedded)\n",
        "\n",
        "            # Run decoder LSTM once for the whole sequence\n",
        "            decoder_outputs, (hidden, cell) = self.decoder.lstm(embedded, (hidden, cell))\n",
        "            # decoder_outputs: (batch, seq_len, hidden_size)\n",
        "            # Map to vocab\n",
        "            pred = self.decoder.fc_out(decoder_outputs)  # (batch, seq_len, vocab_size)\n",
        "\n",
        "            # Place predictions into outputs aligned so that outputs[:, t, :] predicts token at tgt[:, t]\n",
        "            outputs[:, 1:tgt_len, :] = pred  # note: pred corresponds to steps 1..tgt_len-1\n",
        "            # (outputs[:,0,:] stays zeros or you can set it to prediction for <sos> if you want)\n",
        "            return outputs\n",
        "\n",
        "        # Fallback: mixed or no teacher forcing -> step-by-step (c≈©)\n",
        "        decoder_input = tgt[:, 0].unsqueeze(1)  # <sos>\n",
        "        for t in range(1, tgt_len):\n",
        "            prediction, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
        "            outputs[:, t, :] = prediction\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = prediction.argmax(1).unsqueeze(1)\n",
        "            decoder_input = tgt[:, t].unsqueeze(1) if teacher_force else top1\n",
        "\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4007c696",
      "metadata": {
        "id": "4007c696"
      },
      "source": [
        "# ============================================================================\n",
        "# 3. HU·∫§N LUY·ªÜN\n",
        "# ============================================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "49dab472",
      "metadata": {
        "id": "49dab472"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, optimizer, criterion, clip, device):\n",
        "    \"\"\"Hu·∫•n luy·ªán 1 epoch\"\"\"\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for src, src_lengths, tgt in dataloader:\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward\n",
        "        output = model(src, src_lengths, tgt, teacher_forcing_ratio=0.5)\n",
        "\n",
        "        # output: (batch, tgt_len, vocab_size)\n",
        "        # tgt: (batch, tgt_len)\n",
        "\n",
        "        # B·ªè <sos> token v√† flatten\n",
        "        output = output[:, 1:, :].reshape(-1, output.shape[-1])\n",
        "        tgt = tgt[:, 1:].reshape(-1)\n",
        "\n",
        "        # T√≠nh loss\n",
        "        loss = criterion(output, tgt)\n",
        "\n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    \"\"\"ƒê√°nh gi√° tr√™n t·∫≠p validation\"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, src_lengths, tgt in dataloader:\n",
        "            src = src.to(device)\n",
        "            tgt = tgt.to(device)\n",
        "\n",
        "            # Forward (kh√¥ng teacher forcing)\n",
        "            output = model(src, src_lengths, tgt, teacher_forcing_ratio=0)\n",
        "\n",
        "            output = output[:, 1:, :].reshape(-1, output.shape[-1])\n",
        "            tgt = tgt[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, tgt)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9377802",
      "metadata": {
        "id": "e9377802"
      },
      "source": [
        "# ============================================================================\n",
        "# 4. D·ª∞ ƒêO√ÅN (INFERENCE)\n",
        "# ============================================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "801b4f7f",
      "metadata": {
        "id": "801b4f7f"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(model, sentence, src_vocab, tgt_vocab, src_tokenizer, device, max_len=50):\n",
        "    \"\"\"\n",
        "    D·ªãch m·ªôt c√¢u t·ª´ ti·∫øng Anh sang ti·∫øng Ph√°p\n",
        "\n",
        "    Args:\n",
        "        model: m√¥ h√¨nh ƒë√£ train\n",
        "        sentence: c√¢u ti·∫øng Anh (string)\n",
        "        src_vocab, tgt_vocab: t·ª´ ƒëi·ªÉn\n",
        "        src_tokenizer: tokenizer cho ti·∫øng Anh\n",
        "        device: cuda/cpu\n",
        "        max_len: ƒë·ªô d√†i t·ªëi ƒëa c√¢u d·ªãch\n",
        "\n",
        "    Returns:\n",
        "        translated_sentence: c√¢u ti·∫øng Ph√°p (string)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = src_tokenizer(sentence.lower())\n",
        "    tokens = [token.text for token in tokens]\n",
        "\n",
        "    # Encode\n",
        "    indices = src_vocab.encode(tokens) + [src_vocab.eos_idx]\n",
        "    src_tensor = torch.LongTensor(indices).unsqueeze(0).to(device)  # (1, src_len)\n",
        "    src_lengths = torch.LongTensor([len(indices)])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Encoder\n",
        "        hidden, cell = model.encoder(src_tensor, src_lengths)\n",
        "\n",
        "        # Decoder\n",
        "        decoder_input = torch.LongTensor([tgt_vocab.sos_idx]).unsqueeze(0).to(device)\n",
        "        translated_indices = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            prediction, hidden, cell = model.decoder(decoder_input, hidden, cell)\n",
        "            predicted_token = prediction.argmax(1).item()\n",
        "\n",
        "            if predicted_token == tgt_vocab.eos_idx:\n",
        "                break\n",
        "\n",
        "            translated_indices.append(predicted_token)\n",
        "            decoder_input = torch.LongTensor([predicted_token]).unsqueeze(0).to(device)\n",
        "\n",
        "    # Decode\n",
        "    translated_tokens = tgt_vocab.decode(translated_indices)\n",
        "    translated_sentence = ' '.join(translated_tokens)\n",
        "\n",
        "    return translated_sentence\n",
        "\n",
        "\n",
        "def calculate_bleu(model, test_data, src_vocab, tgt_vocab, src_tokenizer, device):\n",
        "    \"\"\"T√≠nh BLEU score tr√™n t·∫≠p test\"\"\"\n",
        "    bleu_scores = []\n",
        "    smoothing = SmoothingFunction().method1\n",
        "\n",
        "    for src_sent, tgt_sent in test_data:\n",
        "        src_text = ' '.join(src_sent)\n",
        "        translated = translate_sentence(model, src_text, src_vocab, tgt_vocab, src_tokenizer, device)\n",
        "\n",
        "        reference = [tgt_sent]\n",
        "        candidate = translated.split()\n",
        "\n",
        "        score = sentence_bleu(reference, candidate, smoothing_function=smoothing)\n",
        "        bleu_scores.append(score)\n",
        "\n",
        "    return np.mean(bleu_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0kd2bXrFMDQ0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kd2bXrFMDQ0",
        "outputId": "a5eacd02-e9e9-4f42-f71f-8c79b023a52f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ ƒêang ƒë·ªçc d·ªØ li·ªáu g·ªëc...\n",
            "T·ªïng s·ªë c√¢u song ng·ªØ: 2,005,688\n"
          ]
        }
      ],
      "source": [
        "# ---------------------\n",
        "# üìÅ ƒê∆∞·ªùng d·∫´n file d·ªØ li·ªáu\n",
        "# ---------------------\n",
        "en_path = r\"D:\\HK7NAM4\\LTSM\\English_French_Machine_Translation_LSTM\\dataset\\fr-en\\europarl-v7.fr-en.en\"\n",
        "fr_path = r\"D:\\HK7NAM4\\LTSM\\English_French_Machine_Translation_LSTM\\dataset\\fr-en\\europarl-v7.fr-en.fr\"\n",
        "\n",
        "# ---------------------\n",
        "# üìÑ H√†m ƒë·ªçc file\n",
        "# ---------------------\n",
        "def load_lines(path):\n",
        "    lines = []\n",
        "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                lines.append(line)\n",
        "    return lines\n",
        "\n",
        "# ---------------------\n",
        "# üîπ ƒê·ªçc d·ªØ li·ªáu g·ªëc\n",
        "# ---------------------\n",
        "print(\"üîπ ƒêang ƒë·ªçc d·ªØ li·ªáu g·ªëc...\")\n",
        "en_lines = load_lines(en_path)\n",
        "fr_lines = load_lines(fr_path)\n",
        "print(f\"T·ªïng s·ªë c√¢u song ng·ªØ: {len(en_lines):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ADJjjRrMN63d",
      "metadata": {
        "id": "ADJjjRrMN63d"
      },
      "source": [
        "1Ô∏è‚É£ D√≤ng tr·ªëng ho·∫∑c whitespace\n",
        "\n",
        "line.strip() lo·∫°i b·ªè kho·∫£ng tr·∫Øng, v√† if line: b·ªè c√°c d√≤ng r·ªóng.\n",
        "\n",
        "N·∫øu file dataset c√≥ nhi·ªÅu d√≤ng tr·ªëng, ch√∫ng s·∫Ω b·ªã lo·∫°i b·ªè.\n",
        "\n",
        "2Ô∏è‚É£ D√≤ng kh√¥ng kh·ªõp gi·ªØa 2 file\n",
        "\n",
        "File EN v√† file FR ph·∫£i c√πng s·ªë d√≤ng, m·ªói d√≤ng t∆∞∆°ng ·ª©ng l√† c·∫∑p c√¢u song ng·ªØ.\n",
        "\n",
        "N·∫øu m·ªôt file c√≥ √≠t d√≤ng h∆°n, b·∫°n ph·∫£i c·∫Øt b·∫±ng nhau b·∫±ng min(len(en_lines), len(fr_lines)).\n",
        "\n",
        "Nh·ªØng d√≤ng th·ª´a ·ªü file d√†i h∆°n s·∫Ω b·ªã lo·∫°i b·ªè.\n",
        "\n",
        "3Ô∏è‚É£ L·ªói encoding / k√Ω t·ª± ƒë·∫∑c bi·ªát\n",
        "\n",
        "B·∫°n d√πng errors='ignore', nghƒ©a l√† nh·ªØng d√≤ng c√≥ k√Ω t·ª± kh√¥ng ƒë·ªçc ƒë∆∞·ª£c s·∫Ω b·ªã b·ªè.\n",
        "\n",
        "Europarl c√≥ nhi·ªÅu k√Ω t·ª± ƒë·∫∑c bi·ªát ho·∫∑c m√£ l·ªói, n√™n nhi·ªÅu d√≤ng b·ªã m·∫•t.\n",
        "\n",
        "4Ô∏è‚É£ Ch·ªâ gi·ªØ nh·ªØng d√≤ng kh√¥ng r·ªóng v√† h·ª£p l·ªá\n",
        "\n",
        "Khi train_test_split ch·∫°y, ch·ªâ nh·ªØng c·∫∑p d√≤ng t·ªìn t·∫°i ·ªü c·∫£ 2 file m·ªõi ƒë∆∞·ª£c d√πng ‚Üí s·ªë l∆∞·ª£ng gi·∫£m m·∫°nh."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1aw4b5oYMFx6",
      "metadata": {
        "id": "1aw4b5oYMFx6"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "EMBEDDING_DIM = 256\n",
        "HIDDEN_SIZE = 512\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.5\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 10\n",
        "CLIP = 1\n",
        "PATIENCE = 3  # cho early stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zn4-pq3jMsqg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn4-pq3jMsqg",
        "outputId": "71e2fd0d-bf8d-4819-9e1a-57bbe083dc4f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'en_lines' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Ch·ªçn s·ªë c√¢u t·ªëi ƒëa b·∫±ng ƒë·ªô d√†i nh·ªè h∆°n\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m min_len = \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43men_lines\u001b[49m), \u001b[38;5;28mlen\u001b[39m(fr_lines))\n\u001b[32m      3\u001b[39m en_lines = en_lines[:min_len]\n\u001b[32m      4\u001b[39m fr_lines = fr_lines[:min_len]\n",
            "\u001b[31mNameError\u001b[39m: name 'en_lines' is not defined"
          ]
        }
      ],
      "source": [
        "# Ch·ªçn s·ªë c√¢u t·ªëi ƒëa b·∫±ng ƒë·ªô d√†i nh·ªè h∆°n\n",
        "min_len = min(len(en_lines), len(fr_lines))\n",
        "en_lines = en_lines[:min_len]\n",
        "fr_lines = fr_lines[:min_len]\n",
        "\n",
        "print(f\"S·ªë c√¢u sau khi ƒë·ªìng b·ªô: {len(en_lines):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5k1fsBWLMJ7o",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5k1fsBWLMJ7o",
        "outputId": "0518bfbb-fa42-40c8-b70c-d3148731ab6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set: 1,804,311 c√¢u\n",
            "Validation set: 100,240 c√¢u\n",
            "Test set: 100,240 c√¢u\n"
          ]
        }
      ],
      "source": [
        "# ---------------------\n",
        "# ‚úÇÔ∏è Chia train / val / test\n",
        "# ---------------------\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_en, temp_en, train_fr, temp_fr = train_test_split(\n",
        "    en_lines, fr_lines, test_size=0.1, random_state=42\n",
        ")\n",
        "val_en, test_en, val_fr, test_fr = train_test_split(\n",
        "    temp_en, temp_fr, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Train set: {len(train_en):,} c√¢u\")\n",
        "print(f\"Validation set: {len(val_en):,} c√¢u\")\n",
        "print(f\"Test set: {len(test_en):,} c√¢u\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q3WjXc13ISMF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3WjXc13ISMF",
        "outputId": "37e3be3b-07a6-4546-901e-8ca119a302b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è en_core_web_sm not found ‚Üí d√πng spacy.blank('en')\n",
            "‚ö†Ô∏è fr_core_news_sm not found ‚Üí d√πng spacy.blank('fr')\n",
            "‚úÖ Tokenizers loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# ---------------------\n",
        "# üî† Tokenizer (SpaCy)\n",
        "# ---------------------\n",
        "import spacy\n",
        "\n",
        "try:\n",
        "    en_tokenizer = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"tagger\"])\n",
        "except Exception:\n",
        "    print(\"‚ö†Ô∏è en_core_web_sm not found ‚Üí d√πng spacy.blank('en')\")\n",
        "    en_tokenizer = spacy.blank(\"en\")\n",
        "\n",
        "try:\n",
        "    fr_tokenizer = spacy.load(\"fr_core_news_sm\", disable=[\"parser\", \"ner\", \"tagger\"])\n",
        "except Exception:\n",
        "    print(\"‚ö†Ô∏è fr_core_news_sm not found ‚Üí d√πng spacy.blank('fr')\")\n",
        "    fr_tokenizer = spacy.blank(\"fr\")\n",
        "\n",
        "print(\"‚úÖ Tokenizers loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8585c5e",
      "metadata": {
        "id": "a8585c5e",
        "outputId": "9b52ecd7-b0d1-44ab-cd60-ae29f500ba4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Defined fast tokenization function.\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def tokenize_lines_fast(lines, tokenizer, n_process=4, batch_size=1000):\n",
        "    \"\"\"\n",
        "    Tokenize nhanh b·∫±ng spaCy.pipe (ƒëa lu·ªìng, √≠t t·ªën RAM)\n",
        "    \"\"\"\n",
        "    tokenized = []\n",
        "    for doc in tqdm(\n",
        "        tokenizer.pipe(lines, n_process=n_process, batch_size=batch_size),\n",
        "        total=len(lines),\n",
        "        desc=\"üîπ Tokenizing\"\n",
        "    ):\n",
        "        toks = [t.text.lower() for t in doc if t.text.strip()]\n",
        "        tokenized.append(toks)\n",
        "    return tokenized\n",
        "\n",
        "print(\"‚úÖ Defined fast tokenization function.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfc9baa6",
      "metadata": {
        "id": "dfc9baa6",
        "outputId": "83f5bfb7-9514-4a16-f55b-695d028c4937"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ Tokenizing train set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üîπ Tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1804311/1804311 [19:25<00:00, 1548.24it/s]\n",
            "üîπ Tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1804311/1804311 [23:45<00:00, 1266.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ Tokenizing validation set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üîπ Tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100240/100240 [01:29<00:00, 1122.63it/s]\n",
            "üîπ Tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100240/100240 [01:47<00:00, 929.23it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing test set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üîπ Tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100240/100240 [01:28<00:00, 1127.70it/s]\n",
            "üîπ Tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100240/100240 [01:43<00:00, 972.57it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done tokenizing all datasets!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"üîπ Tokenizing train set...\")\n",
        "train_en_tok = tokenize_lines_fast(train_en, en_tokenizer)\n",
        "train_fr_tok = tokenize_lines_fast(train_fr, fr_tokenizer)\n",
        "\n",
        "print(\"üîπ Tokenizing validation set...\")\n",
        "val_en_tok = tokenize_lines_fast(val_en, en_tokenizer)\n",
        "val_fr_tok = tokenize_lines_fast(val_fr, fr_tokenizer)\n",
        "\n",
        "print(\"Tokenizing test set...\")\n",
        "test_en_tok = tokenize_lines_fast(test_en, en_tokenizer)\n",
        "test_fr_tok = tokenize_lines_fast(test_fr, fr_tokenizer)\n",
        "\n",
        "print(\"Done tokenizing all datasets!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "708069d3",
      "metadata": {
        "id": "708069d3",
        "outputId": "f4bd33e3-d966-4ea7-cdda-23b663fc152d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ ƒê√£ l∆∞u tokenized dataset th√†nh tokenized_data.pkl\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"tokenized_data.pkl\", \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        \"train_en_tok\": train_en_tok,\n",
        "        \"train_fr_tok\": train_fr_tok,\n",
        "        \"val_en_tok\": val_en_tok,\n",
        "        \"val_fr_tok\": val_fr_tok,\n",
        "        \"test_en_tok\": test_en_tok,\n",
        "        \"test_fr_tok\": test_fr_tok\n",
        "    }, f)\n",
        "\n",
        "print(\"‚úÖ ƒê√£ l∆∞u tokenized dataset th√†nh tokenized_data.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3Vs6-Px8X_6d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Vs6-Px8X_6d",
        "outputId": "e1cc953f-af04-450d-e4d6-561d3bdf47b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded tokenized dataset from Drive!\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"D:/HK7NAM4/LTSM/tokenized_data.pkl\", \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "train_en_tok = data[\"train_en_tok\"]\n",
        "train_fr_tok = data[\"train_fr_tok\"]\n",
        "val_en_tok = data[\"val_en_tok\"]\n",
        "val_fr_tok = data[\"val_fr_tok\"]\n",
        "test_en_tok = data[\"test_en_tok\"]\n",
        "test_fr_tok = data[\"test_fr_tok\"]\n",
        "\n",
        "print(\"‚úÖ Loaded tokenized dataset from Drive!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "vVRevc3UaVW_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVRevc3UaVW_",
        "outputId": "30bb94a2-1d4e-4bfc-9237-c08ec150a4b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ ƒê√£ r√∫t g·ªçn t·∫≠p train xu·ªëng 50,000 c√¢u.\n"
          ]
        }
      ],
      "source": [
        "# --------------------------\n",
        "# üìâ Subset ƒë·ªÉ train nhanh h∆°n\n",
        "# --------------------------\n",
        "subset_size = 50_000  # ho·∫∑c 300_000 n·∫øu GPU b·∫°n ·ªïn\n",
        "\n",
        "train_en_tok = train_en_tok[:subset_size]\n",
        "train_fr_tok = train_fr_tok[:subset_size]\n",
        "\n",
        "# (val v√† test c√≥ th·ªÉ gi·ªØ nguy√™n, v√¨ ch√∫ng nh·ªè)\n",
        "print(f\"‚úÖ ƒê√£ r√∫t g·ªçn t·∫≠p train xu·ªëng {len(train_en_tok):,} c√¢u.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "00cd3cfc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00cd3cfc",
        "outputId": "a1f96a24-d53b-48f6-a9b2-278554bfae99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building vocabularies...\n",
            "English vocab size: 10000\n",
            "French vocab size: 10000\n"
          ]
        }
      ],
      "source": [
        "print(\"Building vocabularies...\")\n",
        "\n",
        "src_vocab = Vocabulary(max_vocab_size=10000)\n",
        "src_vocab.build_vocab(train_en_tok)\n",
        "\n",
        "tgt_vocab = Vocabulary(max_vocab_size=10000)\n",
        "tgt_vocab.build_vocab(train_fr_tok)\n",
        "\n",
        "print(f\"English vocab size: {len(src_vocab)}\")\n",
        "print(f\"French vocab size: {len(tgt_vocab)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "67833d58",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67833d58",
        "outputId": "da25ee99-f8bc-4193-f4d4-7069e5da23c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ Creating datasets...\n",
            "üîπ Building dataloaders...\n",
            "‚úÖ Train examples: 50,000\n",
            "‚úÖ Val examples: 100,240\n",
            "‚úÖ Test examples: 100,240\n",
            "üîπ Example tokenized (src ‚Üí tgt):\n",
            "['i', 'read', 'them', 'both', 'with', 'great', 'interest', '.'] ‚Üí ['je', 'suis', 'fondamentalement', 'r√©ceptif', '√†', 'toutes', 'les', 'tendances', 'lib√©ralisatrices', 'qui', 'rendent', 'possibles', 'de', 'nouvelles', 'formes']\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"üîπ Creating datasets...\")\n",
        "\n",
        "train_dataset = TranslationDataset(train_en_tok, train_fr_tok, src_vocab, tgt_vocab)\n",
        "val_dataset = TranslationDataset(val_en_tok, val_fr_tok, src_vocab, tgt_vocab)\n",
        "test_data = list(zip(test_en_tok, test_fr_tok))  # cho BLEU\n",
        "\n",
        "# hi·ªÉn th·ªã qu√° tr√¨nh kh·ªüi t·∫°o dataloader\n",
        "print(\"üîπ Building dataloaders...\")\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(f\"‚úÖ Train examples: {len(train_dataset):,}\")\n",
        "print(f\"‚úÖ Val examples: {len(val_dataset):,}\")\n",
        "print(f\"‚úÖ Test examples: {len(test_data):,}\")\n",
        "\n",
        "print(\"üîπ Example tokenized (src ‚Üí tgt):\")\n",
        "print(train_en_tok[0][:15], \"‚Üí\", train_fr_tok[0][:15])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "65dddb6e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65dddb6e",
        "outputId": "7d39d23b-e4b8-4877-aed7-ad680c5e1f16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Model initialized and ready for training.\n"
          ]
        }
      ],
      "source": [
        "encoder = Encoder(len(src_vocab), EMBEDDING_DIM, HIDDEN_SIZE, NUM_LAYERS, DROPOUT)\n",
        "decoder = Decoder(len(tgt_vocab), EMBEDDING_DIM, HIDDEN_SIZE, NUM_LAYERS, DROPOUT)\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab.pad_idx)\n",
        "\n",
        "print(\"‚úÖ Model initialized and ready for training.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "7143c688",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7143c688",
        "outputId": "72e9da5d-2038-49e9-fd1c-3967bb96ca41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üåç Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üß† Training:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 417/782 [1:24:25<1:13:53, 12.15s/it, loss=5.681]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[32m     55\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müåç Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLIP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m     val_loss = evaluate(model, val_loader, criterion, device)\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# ‚úÖ K·∫øt qu·∫£ g·ªçn ƒë·∫πp gi·ªëng code c≈©\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, criterion, clip, device)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Loss\u001b[39;00m\n\u001b[32m     22\u001b[39m loss = criterion(output, tgt)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n\u001b[32m     26\u001b[39m optimizer.step()\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\DA\\DataMining\\.venv\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\DA\\DataMining\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\DA\\DataMining\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, criterion, clip, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # leave=True ƒë·ªÉ thanh ti·∫øn tr√¨nh v·∫´n hi·ªán sau khi xong\n",
        "    loop = tqdm(dataloader, desc=\"üß† Training\", leave=True)\n",
        "    for src, src_lengths, tgt in loop:  # Unpack src_lengths\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward (c√≥ teacher forcing)\n",
        "        output = model(src, src_lengths, tgt, teacher_forcing_ratio=0.5)\n",
        "\n",
        "        # output: (batch, tgt_len, vocab_size)\n",
        "        # tgt: (batch, tgt_len)\n",
        "        output = output[:, 1:, :].reshape(-1, output.shape[-1])\n",
        "        tgt = tgt[:, 1:].reshape(-1)\n",
        "\n",
        "        # Loss\n",
        "        loss = criterion(output, tgt)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        loop.set_postfix(loss=f\"{loss.item():.3f}\")\n",
        "\n",
        "    return epoch_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        loop = tqdm(dataloader, desc=\"üîç Evaluating\", leave=True)\n",
        "        for src, src_lengths, tgt in loop:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            output = model(src, src_lengths, tgt, teacher_forcing_ratio=0)\n",
        "            output = output[:, 1:, :].reshape(-1, output.shape[-1])\n",
        "            tgt = tgt[:, 1:].reshape(-1)\n",
        "            loss = criterion(output, tgt)\n",
        "            epoch_loss += loss.item()\n",
        "            loop.set_postfix(loss=f\"{loss.item():.3f}\")\n",
        "    return epoch_loss / len(dataloader)\n",
        "\n",
        "\n",
        "# === MAIN TRAIN LOOP ===\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nüåç Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, criterion, CLIP, device)\n",
        "    val_loss = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    # ‚úÖ K·∫øt qu·∫£ g·ªçn ƒë·∫πp gi·ªëng code c≈©\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}\")\n",
        "\n",
        "    # L∆∞u best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        print(\"üíæ ‚Üí Best model saved!\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(\"‚õî Early stopping!\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c22f724c",
      "metadata": {
        "id": "c22f724c"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "bleu_score = calculate_bleu(model, test_data, src_vocab, tgt_vocab, en_tokenizer, device)\n",
        "print(f\"‚úÖ BLEU Score on test set: {bleu_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ce7a31f",
      "metadata": {
        "id": "8ce7a31f"
      },
      "source": [
        "# ============================================================================\n",
        "# 5. MAIN - S·ª¨ D·ª§NG\n",
        "# ============================================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d113f40b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d113f40b",
        "outputId": "9a7ae1e4-9106-424d-c84e-1a097c3e30b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: fr_core_news_sm not found. Using spacy.blank('fr'). Please install with !python -m spacy download fr_core_news_sm\n",
            "Tokenizing...\n",
            "Building vocabularies...\n",
            "Train examples: 29000\n",
            "Val examples: 1014\n",
            "Test examples: 1071\n",
            "Example tokenized (src -> tgt):\n",
            "['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n",
            "['deux', 'jeunes', 'hommes', 'blancs', 'sont', 'dehors', 'pr√®s', 'de', 'buissons', '.']\n",
            "Epoch 1/30\n",
            "Train Loss: 4.895 | Val Loss: 4.676\n",
            "Epoch 2/30\n",
            "Train Loss: 4.033 | Val Loss: 4.382\n",
            "Epoch 3/30\n",
            "Train Loss: 3.604 | Val Loss: 4.088\n",
            "Epoch 4/30\n",
            "Train Loss: 3.323 | Val Loss: 3.995\n",
            "Epoch 5/30\n",
            "Train Loss: 3.071 | Val Loss: 3.863\n",
            "Epoch 6/30\n",
            "Train Loss: 2.888 | Val Loss: 3.773\n",
            "Epoch 7/30\n",
            "Train Loss: 2.741 | Val Loss: 3.715\n",
            "Epoch 8/30\n",
            "Train Loss: 2.598 | Val Loss: 3.671\n",
            "Epoch 9/30\n",
            "Train Loss: 2.459 | Val Loss: 3.652\n",
            "Epoch 10/30\n",
            "Train Loss: 2.353 | Val Loss: 3.601\n",
            "Epoch 11/30\n",
            "Train Loss: 2.279 | Val Loss: 3.559\n",
            "Epoch 12/30\n",
            "Train Loss: 2.193 | Val Loss: 3.476\n",
            "Epoch 13/30\n",
            "Train Loss: 2.104 | Val Loss: 3.552\n",
            "Epoch 14/30\n",
            "Train Loss: 2.012 | Val Loss: 3.587\n",
            "Epoch 15/30\n",
            "Train Loss: 1.943 | Val Loss: 3.532\n",
            "Early stopping!\n",
            "BLEU Score: 0.1435\n"
          ]
        }
      ],
      "source": [
        "# # Removed the main() function definition and moved the code outside\n",
        "# # This makes the variables (model, vocabularies, tokenizer, device, etc.) available globally.\n",
        "\n",
        "# # C·∫•u h√¨nh\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# # Hyperparameters\n",
        "# EMBEDDING_DIM = 256\n",
        "# HIDDEN_SIZE = 512\n",
        "# NUM_LAYERS = 2\n",
        "# DROPOUT = 0.5\n",
        "# LEARNING_RATE = 0.001\n",
        "# BATCH_SIZE = 64\n",
        "# NUM_EPOCHS = 30\n",
        "# CLIP = 1\n",
        "\n",
        "# # -- ƒê∆∞·ªùng d·∫´n t·ªõi th∆∞ m·ª•c d·ªØ li·ªáu (t∆∞∆°ng ƒë·ªëi v·ªõi notebook) --\n",
        "# data_dir = '/content/drive/MyDrive/Dataset'\n",
        "\n",
        "# def load_lines(path):\n",
        "#     \"\"\"ƒê·ªçc file v√† tr·∫£ v·ªÅ danh s√°ch d√≤ng (kh√¥ng tokenized)\"\"\"\n",
        "#     lines = []\n",
        "#     with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "#         for line in f:\n",
        "#             line = line.strip()\n",
        "#             if line:\n",
        "#                 lines.append(line)\n",
        "#     return lines\n",
        "\n",
        "# def tokenize_lines(lines, tokenizer):\n",
        "#     \"\"\"Tokenize danh s√°ch c√¢u tr·∫£ v·ªÅ list of token lists\"\"\"\n",
        "#     tokenized = []\n",
        "#     for line in lines:\n",
        "#         toks = [t.text for t in tokenizer(line.lower()) if t.text.strip()]\n",
        "#         tokenized.append(toks)\n",
        "#     return tokenized\n",
        "\n",
        "# # T·∫≠p tin theo c·∫•u tr√∫c workspace: dataset/<split>/(file)\n",
        "# train_en_path = os.path.join(data_dir, 'train.en')\n",
        "# train_fr_path = os.path.join(data_dir, 'train.fr')\n",
        "# val_en_path = os.path.join(data_dir, 'val.en')\n",
        "# val_fr_path = os.path.join(data_dir, 'val.fr')\n",
        "# test_en_path = os.path.join(data_dir, 'test_2018_flickr.en')\n",
        "# test_fr_path = os.path.join(data_dir, 'test_2018_flickr.fr')\n",
        "\n",
        "\n",
        "# # Load raw lines\n",
        "# train_en_lines = load_lines(train_en_path)\n",
        "# train_fr_lines = load_lines(train_fr_path)\n",
        "# val_en_lines = load_lines(val_en_path)\n",
        "# val_fr_lines = load_lines(val_fr_path)\n",
        "# test_en_lines = load_lines(test_en_path)\n",
        "# test_fr_lines = load_lines(test_fr_path)\n",
        "\n",
        "# # Tokenizers: th·ª≠ load spacy models, fallback sang spacy.blank n·∫øu model ch∆∞a c√†i\n",
        "# try:\n",
        "#     en_tokenizer = spacy.load('en_core_web_sm')\n",
        "# except Exception:\n",
        "#     # N·∫øu model en_core_web_sm ch∆∞a c√†i, d√πng blank tokenizer (ƒë∆°n gi·∫£n)\n",
        "#     print(\"Warning: en_core_web_sm not found. Using spacy.blank('en'). Please install with !python -m spacy download en_core_web_sm\")\n",
        "#     en_tokenizer = spacy.blank('en')\n",
        "\n",
        "# try:\n",
        "#     fr_tokenizer = spacy.load('fr_core_news_sm')\n",
        "# except Exception:\n",
        "#     print(\"Warning: fr_core_news_sm not found. Using spacy.blank('fr'). Please install with !python -m spacy download fr_core_news_sm\")\n",
        "#     fr_tokenizer = spacy.blank('fr')\n",
        "\n",
        "\n",
        "# # Tokenize t·∫•t c·∫£\n",
        "# print('Tokenizing...')\n",
        "# train_en_tok = tokenize_lines(train_en_lines, en_tokenizer)\n",
        "# train_fr_tok = tokenize_lines(train_fr_lines, fr_tokenizer)\n",
        "# val_en_tok = tokenize_lines(val_en_lines, en_tokenizer)\n",
        "# val_fr_tok = tokenize_lines(val_fr_lines, fr_tokenizer)\n",
        "# test_en_tok = tokenize_lines(test_en_lines, en_tokenizer)\n",
        "# test_fr_tok = tokenize_lines(test_fr_lines, fr_tokenizer)\n",
        "\n",
        "# # Build vocabularies (t·ª´ train set)\n",
        "# print('Building vocabularies...')\n",
        "# src_vocab = Vocabulary(max_vocab_size=10000)\n",
        "# src_vocab.build_vocab(train_en_tok)\n",
        "# tgt_vocab = Vocabulary(max_vocab_size=10000)\n",
        "# tgt_vocab.build_vocab(train_fr_tok)\n",
        "\n",
        "# # T·∫°o Dataset v√† DataLoader\n",
        "# train_dataset = TranslationDataset(train_en_tok, train_fr_tok, src_vocab, tgt_vocab)\n",
        "# val_dataset = TranslationDataset(val_en_tok, val_fr_tok, src_vocab, tgt_vocab)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# # test_data d√πng cho calculate_bleu (d·∫°ng list of (src_tokens, tgt_tokens))\n",
        "# test_data = list(zip(test_en_tok, test_fr_tok))\n",
        "\n",
        "# # Sanity prints\n",
        "# print(f'Train examples: {len(train_dataset)}')\n",
        "# print(f'Val examples: {len(val_dataset)}')\n",
        "# print(f'Test examples: {len(test_data)}')\n",
        "# print('Example tokenized (src -> tgt):')\n",
        "# print(train_en_tok[0][:20])\n",
        "# print(train_fr_tok[0][:20])\n",
        "\n",
        "# # Kh·ªüi t·∫°o m√¥ h√¨nh (d√πng k√≠ch th∆∞·ªõc t·ª´ vocab ƒë√£ t·∫°o)\n",
        "# encoder = Encoder(len(src_vocab), EMBEDDING_DIM, HIDDEN_SIZE, NUM_LAYERS, DROPOUT)\n",
        "# decoder = Decoder(len(tgt_vocab), EMBEDDING_DIM, HIDDEN_SIZE, NUM_LAYERS, DROPOUT)\n",
        "# model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "# # Optimizer v√† loss\n",
        "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "# # Use ignore_index for PAD_TOKEN in CrossEntropyLoss\n",
        "# criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab.pad_idx)\n",
        "\n",
        "\n",
        "# # (Ph·∫ßn training v·∫´n nh∆∞ c≈©)\n",
        "# best_val_loss = float('inf')\n",
        "# patience = 3\n",
        "# patience_counter = 0\n",
        "\n",
        "# for epoch in range(NUM_EPOCHS):\n",
        "#     train_loss = train_epoch(model, train_loader, optimizer, criterion, CLIP, device)\n",
        "#     val_loss = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "#     print(f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
        "#     print(f'Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}')\n",
        "\n",
        "#     if val_loss < best_val_loss:\n",
        "#         best_val_loss = val_loss\n",
        "#         patience_counter = 0\n",
        "#         torch.save(model.state_dict(), 'best_model.pth')\n",
        "#     else:\n",
        "#         patience_counter += 1\n",
        "#         if patience_counter >= patience:\n",
        "#             print(\"Early stopping!\")\n",
        "#             break\n",
        "\n",
        "# bleu_score = calculate_bleu(model, test_data, src_vocab, tgt_vocab, en_tokenizer, device)\n",
        "# print(f'BLEU Score: {bleu_score:.4f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bW4znrDsw5c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW4znrDsw5c8",
        "outputId": "7e374913-054e-4dbb-f27e-40c172f137dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English:  A man is riding a horse.\n",
            "French: un homme est sur cheval cheval .\n"
          ]
        }
      ],
      "source": [
        "sentence = \" A man is riding a horse.\"\n",
        "translation = translate_sentence(model, sentence, src_vocab, tgt_vocab, en_tokenizer, device)\n",
        "print(f'English: {sentence}')\n",
        "print(f'French: {translation}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Mh3VsH4KZINW",
      "metadata": {
        "id": "Mh3VsH4KZINW"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (.venv)",
      "language": "python",
      "name": ".venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
